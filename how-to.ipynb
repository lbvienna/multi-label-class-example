{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is modeled after the classifier that we used for this story: https://www.washingtonpost.com/graphics/politics/policy-2020/priorities-issues/. It's an example of multi-class, multi-label classification. It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "import data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, labels = data_utils.get_texts_and_labels('facebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brothers and sisters: We're going to win this election not because we have a super PAC funded by billionaires.\n",
      "('corporate power', 'campaign finance')\n"
     ]
    }
   ],
   "source": [
    "print(texts[312])\n",
    "print(labels[312])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_binarizer(labels):\n",
    "    # since this problem is multi label\n",
    "    binarizer = MultiLabelBinarizer()\n",
    "    return binarizer.fit(labels)\n",
    "\n",
    "def create_featurizer(corpus):\n",
    "    # I like using unigrams and bigrams\n",
    "    featurizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))\n",
    "    return featurizer.fit(corpus)\n",
    "\n",
    "def create_classifier(hyperparameters):\n",
    "    # create a classifier with with specific hyperparameters, otherwise use default hyperparameters\n",
    "    if hyperparameters is not None and 'estimator__alpha' in hyperparameters:\n",
    "        base_classifier = SGDClassifier(loss='modified_huber', penalty='elasticnet', tol=1e-3, alpha=hyperparameters['estimator__alpha'])\n",
    "    else:\n",
    "        base_classifier = SGDClassifier(loss='modified_huber', penalty='elasticnet', tol=1e-3)\n",
    "    return OneVsRestClassifier(base_classifier, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For basic text classification TFIDF features are good and I have found that adding bigrams often boosts the performance -- while adding any further n-grams usually doesn't help much, but instead massively increases the feature space.\n",
    "\n",
    "I have also found that Supper Vector Machine (SVM) classifiers preform well on NLP tasks. Unfortunately `sklearn.svm.SVC` and `sklearn.svm.LinearSVC` often converge quite slowly, especially with large amounts of data since the usual SVM solvers scale at `O(n^2)` (where `n` is the size of the training set). So I generally prefer the `sklearn.linear_model.SGDClassifier`. If the specified loss is set to `hinge`, then it solves the exact same optimization problem as a SVM, except using stochastic gradient descent, which scales linearly with the size of training set and is therefore a lot faster. \n",
    "\n",
    "We now lose the ability to use non-linear kernels in the SVM (though we theoretically could use the Nystroem kernel approximation with `sklearn.kernel_approximation.Nystroem`) but for NLP problems this is less of an issue. Since the features are sparse, the kernel trick generally doesn't help.\n",
    "\n",
    "Also note that I use the `modified_huber` instead of `hinge` loss. These losses are quite similar, but I prefer the former since it allows for predicting probabilities and not just outcomes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = create_binarizer(labels)\n",
    "featurizer = create_featurizer(texts)\n",
    "\n",
    "tfidf = featurizer.transform(texts)\n",
    "binarized_labels = mlb.transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train, tfidf_test, binarized_labels_train, binarized_labels_test = train_test_split(tfidf, binarized_labels, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 29192)\t0.2867602968749451\n",
      "  (0, 29191)\t0.20563101243820897\n",
      "  (0, 25712)\t0.2597172020626997\n",
      "  (0, 25709)\t0.24389800569408934\n",
      "  (0, 24446)\t0.2867602968749451\n",
      "  (0, 24442)\t0.2239681752576899\n",
      "  (0, 19066)\t0.2867602968749451\n",
      "  (0, 19065)\t0.24389800569408934\n",
      "  (0, 11552)\t0.24389800569408934\n",
      "  (0, 11429)\t0.12156689082737338\n",
      "  (0, 11000)\t0.2867602968749451\n",
      "  (0, 10998)\t0.2239681752576899\n",
      "  (0, 8880)\t0.2867602968749451\n",
      "  (0, 8869)\t0.19892628518558983\n",
      "  (0, 3475)\t0.23267410725045434\n",
      "  (0, 3471)\t0.22024965442790256\n",
      "  (0, 3064)\t0.2081489788890795\n",
      "[0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf[312])\n",
    "print(binarized_labels[312])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(hyperparameters):\n",
    "    classifier = create_classifier(hyperparameters)\n",
    "    classifier.fit(tfidf_train, binarized_labels_train)\n",
    "    y_pred = classifier.predict(tfidf_test)\n",
    "    f1 = f1_score(binarized_labels_test, y_pred, average='micro')\n",
    "    precision = precision_score(binarized_labels_test, y_pred, average='micro')\n",
    "    recall = recall_score(binarized_labels_test, y_pred, average='micro')\n",
    "    print(f\"precision: {precision}\")\n",
    "    print(f\"recall: {recall}\")\n",
    "    print(f\"f1: {f1}\")\n",
    "\n",
    "def find_best_hyperparmeters(tfidf, labels):\n",
    "    classifier = create_classifier(None)\n",
    "    # we have to specify `estimator__` since the classifier is embedded in the `oneVsRest` wrapper.\n",
    "    param_distribution = {\n",
    "                        'estimator__alpha': scipy.stats.expon(scale=0.00001),\n",
    "    }\n",
    "    # this runs search on maximum available cores\n",
    "    extra_kwargs = {'n_jobs': -1}\n",
    "    scv = RandomizedSearchCV(classifier, param_distribution, n_iter=20, cv=5, scoring='f1_micro', iid=True, verbose=1, refit=False, **extra_kwargs)\n",
    "    scv.fit(tfidf, labels)\n",
    "    return scv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I prefer random search over hyperparameters to grid search. It's generally more useful if you have more than one hyperparameter. Here is a good explainer: http://cs230.stanford.edu/section/8/. While this is written about neural networks specifically, it also holds for less complication machine learning algorithms.\n",
    "\n",
    "I have found that an exponential distribution with scale set at `1e-5` generally works quite well as distribution to sample the hyperparameter for an `SGDClassifier` from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.9085714285714286\n",
      "recall: 0.2804232804232804\n",
      "f1: 0.42857142857142855\n"
     ]
    }
   ],
   "source": [
    "test_classifier(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see precision is very high, but our recall is suffering. High precision means the labels that our algorithm returned are mostly correct, and the low recall means that we are missing a lot of the true labels. Generally, this means that our classifier is not very confident since it is only tagging the \"easy\" examples.\n",
    "\n",
    "One way to try and improve our score is to use hyperparameter optimization. Many machine learning algorithms rely on hyperparameters that are set before training, which can heavily affect the classification. For an `SGDClassifier` the most interesting hyperparameter is `alpha`, which controls the strength of the regularization (how strongly we are forcing the parameters to zero, which tries to combat overfitting) and defaults to `0.0001`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    1.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best hyperparmeter are:  {'estimator__alpha': 9.66253922334479e-07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    4.1s finished\n"
     ]
    }
   ],
   "source": [
    "# first test classifier to see how it would perform without hyperparameter optimization\n",
    "best_hyperparameters = find_best_hyperparmeters(tfidf_train, binarized_labels_train)\n",
    "# then test with hyperparmeters\n",
    "print(\"best hyperparmeter are: \", best_hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the optimal hyperparemter we found for alpha is substantially smaller than the default setting. This means we were initially underfitting our model, since we were forcing the parameters closer to zero than the data wanted them to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.8161290322580645\n",
      "recall: 0.4462081128747795\n",
      "f1: 0.5769669327251995\n"
     ]
    }
   ],
   "source": [
    "test_classifier(best_hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The f1 score improved quite substantially. This due to a large increase in recall (and a correspondingly small decrease in precision. We can see the hyperparameter tuning worked. \n",
    "\n",
    "Since we were forcing the parameters to zero, the features had to be very indicative of a topic for the model to assign the topic. This means were missing a lot of potential correct predictions. By losening the regularization, and giving the model more flexibility, we were able to increase the recall without losing too much precision. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=SGDClassifier(alpha=1.3834633267768954e-06,\n",
       "                                            average=False, class_weight=None,\n",
       "                                            early_stopping=False, epsilon=0.1,\n",
       "                                            eta0=0.0, fit_intercept=True,\n",
       "                                            l1_ratio=0.15,\n",
       "                                            learning_rate='optimal',\n",
       "                                            loss='modified_huber',\n",
       "                                            max_iter=1000, n_iter_no_change=5,\n",
       "                                            n_jobs=None, penalty='elasticnet',\n",
       "                                            power_t=0.5, random_state=None,\n",
       "                                            shuffle=True, tol=0.001,\n",
       "                                            validation_fraction=0.1, verbose=0,\n",
       "                                            warm_start=False),\n",
       "                    n_jobs=-1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = create_classifier(best_hyperparameters)\n",
    "classifier.fit(tfidf_train, binarized_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('campaign finance', 'corporate power')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text = texts[312]\n",
    "test_text_tfidf = featurizer.transform([test_text])\n",
    "test_text_pred = classifier.predict(test_text_tfidf)\n",
    "mlb.inverse_transform(test_text_pred)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
